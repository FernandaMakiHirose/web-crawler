# Desenvolvendo um Web Crawler
- Web crawler é uma ferramenta usada para encontrar, ler e indexar páginas de um site. É como um robô que captura informações de cada um dos links que encontra pela frente, cadastra e compreende o que é mais relevante. (palavras - chaves)
- Muito utilizado em Levantamento de Informações em um Processo de Pentest

## Requisitos
- Python
- Visual Studio Code ou Terminal
- Instalar bibliotecas

## Licença
Distribuido sob a licença MIT License. Veja `LICENSE` para mais informações

## Executar o projeto
No path `\web-crawler` (Substitua o `nomedoarquivo` pelo nome do arquivo que você quer executar):
>python nomedoarquivo.py

## Bibliotecas
- [BeautifulSoup](https://www.ti-enxame.com/pt/python-3.x/como-instalar-o-beautifulsoup-em-python3-quando-dir-padrao-e-python2.7/822820620/) - é uma biblioteca de extração de dados de arquivos HTML e XML
- [requests](https://docs.python-requests.org/pt_BR/latest/user/install.html)
- operator - exporta um conjunto de funções eficientes correspondentes aos operadores intrínsecos do Python como: + - * / not and
- collections - nos ajuda a preencher e manipular eficientemente as estruturas de dados como tuplas, dicionários e listas


